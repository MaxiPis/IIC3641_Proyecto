{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "X4MBlhmwNhXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cargar dataset"
      ],
      "metadata": {
        "id": "v-ZXz2NUW1K0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn4xGi0RNfJ6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"Siniestros_urbanos_Metropolitana_2024.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(\"Filas y columnas:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Columnas a usar"
      ],
      "metadata": {
        "id": "mhyR4RIcVUh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols_base = [\n",
        "    \"id_accidente\",\n",
        "    \"fecha\",\n",
        "    \"hora\",\n",
        "    \"latitud\",\n",
        "    \"longitud\",\n",
        "    \"n_heridos\",\n",
        "    \"n_fallecidos\",\n",
        "    \"tipo_accidente\",\n",
        "    \"calle\", # ojo: seria la intersección calle1 - calle2\n",
        "    \"dist_hospital\",\n",
        "    \"tiempo_hospital\"\n",
        "]\n",
        "\n",
        "df = df[cols_base]\n",
        "df = df.dropna(subset=[\"latitud\", \"longitud\"])\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "UQOQgCJDVWSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gravedad"
      ],
      "metadata": {
        "id": "bTpxESptVtkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrica simple de gravedad (como en el pdf)\n",
        "df[\"gravedad_accidente\"] = 3*df[\"n_fallecidos\"] + 2*df[\"n_heridos\"]\n",
        "df[\"gravedad_accidente\"].describe()\n",
        "\n",
        "agg_df = df.groupby(\"calle\").agg(\n",
        "    n_accidentes=(\"id_accidente\", \"count\"),\n",
        "    total_heridos=(\"n_heridos\", \"sum\"),\n",
        "    total_fallecidos=(\"n_fallecidos\", \"sum\"),\n",
        "    gravedad_total=(\"gravedad_accidente\", \"sum\"),\n",
        "    dist_prom=(\"dist_hospital\", \"mean\"),\n",
        "    tiempo_prom=(\"tiempo_hospital\", \"mean\")\n",
        ").reset_index()\n",
        "\n",
        "agg_df.head()"
      ],
      "metadata": {
        "id": "eN830awGVuTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## métricas de \"peligrosidad\""
      ],
      "metadata": {
        "id": "g2trDDaOV0WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#0 = calles seguras\n",
        "#1 = riesgo medio\n",
        "#2 = muy peligrosas\n",
        "\n",
        "p1 = np.percentile(agg_df[\"gravedad_total\"], 30)\n",
        "p2 = np.percentile(agg_df[\"gravedad_total\"], 80)\n",
        "\n",
        "def clasificar_peligrosidad(g):\n",
        "    if g <= p1:\n",
        "        return 0\n",
        "    elif g <= p2:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "agg_df[\"peligrosidad\"] = agg_df[\"gravedad_total\"].apply(clasificar_peligrosidad)\n",
        "agg_df[\"peligrosidad\"].value_counts()\n"
      ],
      "metadata": {
        "id": "p8JTwUvuV2CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ver como se comportan las clases"
      ],
      "metadata": {
        "id": "QEuxFcxeV_Xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "agg_df[\"peligrosidad\"].value_counts().sort_index().plot(\n",
        "    kind=\"bar\", color=[\"green\", \"orange\", \"red\"],\n",
        "    title=\"Distribución de clases de peligrosidad (0=Baja, 1=Media, 2=Alta)\"\n",
        ")\n",
        "plt.xlabel(\"Categoría\")\n",
        "plt.ylabel(\"Número de calles\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XE5_T7kvWBee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## limpiamos dataset"
      ],
      "metadata": {
        "id": "H0aIPQFlWxnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Cargar el dataset limpio del punto anterior (si no existe en memoria)\n",
        "path = \"Siniestros_urbanos_Metropolitana_2024.csv\"\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "# Eliminar duplicados y registros con coordenadas faltantes o fuera de rango\n",
        "df = df.drop_duplicates(subset=[\"id_accidente\"])\n",
        "df = df.dropna(subset=[\"latitud\", \"longitud\"])\n",
        "df = df[(df[\"latitud\"].between(-34.5, -33.0)) & (df[\"longitud\"].between(-71.0, -69.0))]\n",
        "\n",
        "# Convertir fechas y horas a formato datetime\n",
        "if \"fecha\" in df.columns and \"hora\" in df.columns:\n",
        "    df[\"fecha_hora\"] = pd.to_datetime(df[\"fecha\"] + \" \" + df[\"hora\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"fecha_hora\"])\n",
        "\n",
        "# Validar tipos numéricos\n",
        "cols_numericas = [\"n_heridos\", \"n_fallecidos\", \"dist_hospital\", \"tiempo_hospital\"]\n",
        "for c in cols_numericas:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0)\n",
        "\n",
        "print(\"Datos limpios:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "qbo9SUs7Wy0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Armar grafo"
      ],
      "metadata": {
        "id": "Snb8nVUqXnHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "grafo de calles"
      ],
      "metadata": {
        "id": "bI5r2qGtXo6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install osmnx"
      ],
      "metadata": {
        "id": "WNAKLLy9YkLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "\n",
        "G = ox.graph_from_place(\"Santiago Metropolitan Region, Chile\",\n",
        "                        network_type=\"drive\", simplify=True)\n",
        "G_proj = ox.project_graph(G)\n",
        "nodes_gdf, edges_gdf = ox.graph_to_gdfs(G_proj, nodes=True, edges=True)\n",
        "edges_gdf = edges_gdf.reset_index().reset_index().rename(columns={\"index\": \"segment_id\"})\n",
        "edges_gdf[[\"segment_id\", \"u\", \"v\", \"key\", \"name\"]].head()"
      ],
      "metadata": {
        "id": "JjjttEHQXoXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "accidentes a tramos"
      ],
      "metadata": {
        "id": "qkftssj_X2Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---\n",
        "# 3.2 Convertir accidentes a GeoDataFrame y asignar tramo más cercano\n",
        "# ---\n",
        "\n",
        "from shapely.geometry import Point\n",
        "import numpy as np\n",
        "acc_gdf = gpd.GeoDataFrame(\n",
        "    df.copy(),\n",
        "    geometry=gpd.points_from_xy(df[\"longitud\"], df[\"latitud\"]),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "acc_gdf = acc_gdf.to_crs(edges_gdf.crs)\n",
        "X = acc_gdf.geometry.x.values\n",
        "Y = acc_gdf.geometry.y.values\n",
        "\n",
        "nearest = ox.distance.nearest_edges(G_proj, X, Y)\n",
        "nearest_uvk = pd.DataFrame(nearest, columns=[\"u\", \"v\", \"key\"])\n",
        "edge_keys = edges_gdf[[\"u\", \"v\", \"key\", \"segment_id\"]].copy()\n",
        "\n",
        "acc_gdf = pd.concat([acc_gdf.reset_index(drop=True), nearest_uvk], axis=1)\n",
        "acc_gdf = acc_gdf.merge(edge_keys, on=[\"u\", \"v\", \"key\"], how=\"left\")\n",
        "\n",
        "acc_gdf[[\"id_accidente\", \"calle\", \"segment_id\"]].head()\n"
      ],
      "metadata": {
        "id": "2JZeZe6rX3XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grafo de calles G"
      ],
      "metadata": {
        "id": "jah8MywaYAC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "#Cada nodo representa un tramo de calle (segment_id).\n",
        "#Dos nodos estarán conectados si COMPARTEN un nodo extremo (u o v)\n",
        "\n",
        "G_segments = nx.Graph()\n",
        "\n",
        "for _, row in edges_gdf.iterrows():\n",
        "    G_segments.add_node(\n",
        "        int(row[\"segment_id\"]),\n",
        "        name=row.get(\"name\", None),\n",
        "        length=row.get(\"length\", None),\n",
        "        highway=row.get(\"highway\", None),\n",
        "        oneway=row.get(\"oneway\", None)\n",
        "    )\n",
        "\n",
        "for node_id, edges in edges_gdf.groupby(\"u\"):\n",
        "    segments = edges[\"segment_id\"].tolist()\n",
        "    for i in range(len(segments)):\n",
        "        for j in range(i + 1, len(segments)):\n",
        "            G_segments.add_edge(int(segments[i]), int(segments[j]))\n",
        "\n",
        "for node_id, edges in edges_gdf.groupby(\"v\"):\n",
        "    segments = edges[\"segment_id\"].tolist()\n",
        "    for i in range(len(segments)):\n",
        "        for j in range(i + 1, len(segments)):\n",
        "            G_segments.add_edge(int(segments[i]), int(segments[j]))\n",
        "\n",
        "print(\"Nodos:\", G_segments.number_of_nodes(), \" | Aristas:\", G_segments.number_of_edges())\n"
      ],
      "metadata": {
        "id": "LEYoR4wYYBPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "agregar features por tramo"
      ],
      "metadata": {
        "id": "HQgk7LB7YLvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg_features = acc_gdf.groupby(\"segment_id\").agg(\n",
        "    n_accidentes=(\"id_accidente\", \"count\"),\n",
        "    total_heridos=(\"n_heridos\", \"sum\"),\n",
        "    total_fallecidos=(\"n_fallecidos\", \"sum\"),\n",
        "    dist_prom=(\"dist_hospital\", \"mean\"),\n",
        "    tiempo_prom=(\"tiempo_hospital\", \"mean\"),\n",
        ").reset_index()\n",
        "\n",
        "# Calcular índice de gravedad y peligrosidad\n",
        "agg_features[\"gravedad_total\"] = (\n",
        "    3 * agg_features[\"total_fallecidos\"] + 2 * agg_features[\"total_heridos\"]\n",
        ")\n",
        "\n",
        "p1 = np.percentile(agg_features[\"gravedad_total\"], 30)\n",
        "p2 = np.percentile(agg_features[\"gravedad_total\"], 80)\n",
        "\n",
        "def peligrosidad_cat(g):\n",
        "    if g <= p1:\n",
        "        return 0\n",
        "    elif g <= p2:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "agg_features[\"peligrosidad\"] = agg_features[\"gravedad_total\"].apply(peligrosidad_cat)\n",
        "agg_features.head()\n"
      ],
      "metadata": {
        "id": "myPyzCUAYNPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_features = agg_features.set_index(\"segment_id\").to_dict(orient=\"index\")\n",
        "\n",
        "# añadimos atributos al grafo\n",
        "nx.set_node_attributes(G_segments, node_features)\n",
        "for node in G_segments.nodes():\n",
        "    G_segments.nodes[node][\"grado\"] = G_segments.degree[node]\n",
        "first_node = list(G_segments.nodes())[0]\n",
        "print(G_segments.nodes[first_node])"
      ],
      "metadata": {
        "id": "jHD9AMrQYVnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualización del grafo"
      ],
      "metadata": {
        "id": "8AWo88bZYbYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nodes_with_label = [n for n, d in G_segments.nodes(data=True) if \"peligrosidad\" in d]\n",
        "\n",
        "color_map = []\n",
        "for n in nodes_with_label:\n",
        "    p = G_segments.nodes[n][\"peligrosidad\"]\n",
        "    color_map.append({0: \"green\", 1: \"orange\", 2: \"red\"}.get(p, \"gray\"))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(\n",
        "    G_segments.subgraph(nodes_with_label),\n",
        "    node_color=color_map,\n",
        "    node_size=10,\n",
        "    with_labels=False\n",
        ")\n",
        "plt.title(\"Grafo de calles coloreado por peligrosidad\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-gpTB9-eYcqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparar datos para GraphSAGE"
      ],
      "metadata": {
        "id": "Z5zxf31uYrMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pasar de networkx a pytorch geometric"
      ],
      "metadata": {
        "id": "eGSaC6-VYvym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "feature_cols = [\n",
        "    \"n_accidentes\", \"total_heridos\", \"total_fallecidos\",\n",
        "    \"dist_prom\", \"tiempo_prom\", \"gravedad_total\", \"grado\"\n",
        "]\n",
        "\n",
        "for n in G_segments.nodes:\n",
        "    for c in feature_cols:\n",
        "        G_segments.nodes[n][c] = G_segments.nodes[n].get(c, 0) or 0\n",
        "data = from_networkx(G_segments)\n",
        "\n",
        "X = torch.tensor(\n",
        "    [[G_segments.nodes[n][c] for c in feature_cols] for n in G_segments.nodes],\n",
        "    dtype=torch.float\n",
        ")\n",
        "y = torch.tensor(\n",
        "    [G_segments.nodes[n].get(\"peligrosidad\", 0) for n in G_segments.nodes],\n",
        "    dtype=torch.long\n",
        ")\n",
        "\n",
        "data.x = X\n",
        "data.y = y\n",
        "\n",
        "print(data)\n",
        "print(\"Shape X:\", data.x.shape, \"  Shape y:\", data.y.shape)\n"
      ],
      "metadata": {
        "id": "7D8OK-T1Y1Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "entrenamiento, validación y test"
      ],
      "metadata": {
        "id": "IxZQ9IxPY9gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "num_nodes = data.num_nodes\n",
        "indices = list(range(num_nodes))\n",
        "\n",
        "train_idx, test_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)\n",
        "\n",
        "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "data.val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "data.train_mask[train_idx] = True\n",
        "data.val_mask[val_idx] = True\n",
        "data.test_mask[test_idx] = True\n",
        "\n",
        "print(f\"Train: {data.train_mask.sum()} | Val: {data.val_mask.sum()} | Test: {data.test_mask.sum()}\")\n"
      ],
      "metadata": {
        "id": "-RBCvsgAZBhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE + MLP (esta es la parte importante del cuaderno)"
      ],
      "metadata": {
        "id": "GmUdR5ctZ4pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class GraphSAGEClassifier(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Dos capas GraphSAGE\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Clasificador MLP, lo haré simple por ahora\n",
        "        self.lin1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.lin2 = torch.nn.Linear(hidden_channels, out_channels)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # 1ra capa GraphSAGE\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # 2da capa GraphSAGE\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # MLP\n",
        "        x = self.lin1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "# instancia de modelo\n",
        "in_channels = data.x.size(1)   # n de features x nodo\n",
        "hidden_channels = 64           # R^64 como en el graphical abstract\n",
        "out_channels = 3               # las 3 clases de peligrosidad\n",
        "\n",
        "model = GraphSAGEClassifier(in_channels, hidden_channels, out_channels)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "DRnqQqpCZ6YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "entrenamiento y validación"
      ],
      "metadata": {
        "id": "xkAUXgKQaNNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# ojo que CrossEntropyLoss ya se encarga de hacer el softmax\n",
        "# el loss ya hace log_softmax + NLLLoss\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(mask):\n",
        "    model.eval()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    preds = out[mask].argmax(dim=1)\n",
        "    y_true = data.y[mask]\n",
        "    acc = (preds == y_true).float().mean().item()\n",
        "    return acc, preds, y_true\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    loss = train()\n",
        "    train_acc, _, _ = evaluate(data.train_mask)\n",
        "    val_acc, _, _ = evaluate(data.val_mask)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "z6s9D0wiaOWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metricas"
      ],
      "metadata": {
        "id": "8nbKhJGnaTKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "test_acc, test_preds, test_true = evaluate(data.test_mask)\n",
        "print(f\"Test Accuracy: {test_acc:.3f}\\n\")\n",
        "\n",
        "test_preds_np = test_preds.cpu().numpy()\n",
        "test_true_np = test_true.cpu().numpy()\n",
        "\n",
        "print(\"Reporte de clasificación (0=Baja, 1=Media, 2=Alta):\")\n",
        "print(classification_report(test_true_np, test_preds_np, digits=3))\n",
        "\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(test_true_np, test_preds_np))"
      ],
      "metadata": {
        "id": "j1PmOl2raUTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}